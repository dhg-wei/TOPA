{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebdbbd25-038d-4987-ac84-d5adbdcd40ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460fe558-fca6-4906-93f3-facd215d9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_arguments(args, filepath):\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(vars(args), file)\n",
    "\n",
    "def load_arguments(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        args_dict = json.load(file)\n",
    "    return args_dict\n",
    "\n",
    "# Optionally, repopulate argparse.ArgumentParser with these arguments\n",
    "def repopulate_arguments(args_dict):\n",
    "    parser = argparse.ArgumentParser(description=\"Example script\")\n",
    "    for key, value in args_dict.items():\n",
    "        parser.add_argument(f'--{key}', type=type(value),default=value)\n",
    "    return parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c50017-a3cf-4fa3-8a7f-2e24c501a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../vqa_checkpoint/checkpoint_pretrain/llama2_7b_acc4_br5e3_correct_vnips'\n",
    "\n",
    "loaded_args = load_arguments(path+'/args.json')\n",
    "\n",
    "args = repopulate_arguments(loaded_args)\n",
    "args.llama_model_path = '.' +args.llama_model_path\n",
    "args.resume='../vqa_checkpoint/checkpoint_pretrain/llama2_7b_acc4_br5e3_correct_vnips/checkpoint_19.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "074499b9-b268-4a4e-b05f-9e0f6d5a189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '../vqa_checkpoint/checkpoint_pretrain/llama2_13b_acc8_br8e3_bs4_vnips'\n",
    "\n",
    "# loaded_args = load_arguments(path+'/argsa.json')\n",
    "\n",
    "# args = repopulate_arguments(loaded_args)\n",
    "# args.llama_model_path = '.' +args.llama_model_path\n",
    "# args.resume='../vqa_checkpoint/checkpoint_pretrain/llama2_13b_acc8_br8e3_bs4_vnips/checkpoint_18.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98596d9e-0760-4eef-995f-7e2d0e47d33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/nus/idmwyk/scratch/anaconda3/envs/llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from llama import Tokenizer\n",
    "from llama_vqa import LLaMA_VQA\n",
    "from dataloader import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9272202e-d5ed-4645-8411-98463087b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'], strict=False)\n",
    "tokenizer = Tokenizer(model_path=f'{args.llama_model_path}./tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b12971-d47f-4aef-ba77-4d1d163160d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e125471-d745-4f57-b51a-b311df9de478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80ba3180-0ce1-4f7c-817f-79aa02c0ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(model, tokenizer, prompt1,prompt2,video=None):\n",
    "    adapter = model.adapter_query.weight.reshape(-1, model.adapter_len, model.params.dim).unsqueeze(1)\n",
    "    freqs= model.freqs_cis.cuda()\n",
    "    \n",
    "    tokens = [tokenizer.bos_id] + tokenizer.sp_model.encode(prompt1)\n",
    "    query = torch.tensor(tokens, dtype=torch.int64).cuda()\n",
    "    input_embedding = model.tok_embeddings(query)\n",
    "\n",
    "    tokens_2 = tokenizer.sp_model.encode(prompt2)\n",
    "    query_2 = torch.tensor(tokens_2, dtype=torch.int64).cuda()\n",
    "    input_embedding_2 = model.tok_embeddings(query_2)\n",
    "    tokens.extend(tokens_2)\n",
    "    video = video.cuda().float()\n",
    "    video/=video.norm(dim=-1,keepdim=True)\n",
    "    if True:\n",
    "        sim = video@model.memory.T\n",
    "\n",
    "        sim = (sim*100).softmax(dim=-1)\n",
    "\n",
    "        video = sim@model.memory\n",
    "        video = video/video.norm(dim=-1,keepdim=True)\n",
    "        \n",
    "    video_feature = model.visual_proj(video)\n",
    "    video_feature = (video_feature + model.temporal_emb.weight[:, :]).type(model.llamatype)\n",
    "    vqa_video_start=input_embedding.shape[0]\n",
    "    # print(video_feature.shape)\n",
    "    input_embedding = torch.cat([input_embedding,video_feature,input_embedding_2])\n",
    "    start_pos=0\n",
    "    for j in range(10):\n",
    "        vqa_h = input_embedding.unsqueeze(0)\n",
    "        seqlen = vqa_h.shape[-2]\n",
    "        freqs_cis = freqs[:seqlen]\n",
    "        mask = None\n",
    "        mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=vqa_h.device)\n",
    "        mask = torch.triu(mask, diagonal=0 + 1).type_as(vqa_h)\n",
    "\n",
    "        for i, layer in enumerate(model.layers[-1 * model.adapter_layer:]):\n",
    "            vqa_h = layer(vqa_h, start_pos, freqs_cis, mask, adapter[i].type(model.llamatype), vqa_video_start)\n",
    "        vqa_h = model.norm(vqa_h)\n",
    "        vqa_output = model.output(vqa_h)\n",
    "        vqa_output = vqa_output.reshape(-1, model.vocab_size)\n",
    "        vqa_output[-1,920]=-100\n",
    "        vqa_output[-1,1128]=-100\n",
    "        next_token = vqa_output[-1,:].argmax()\n",
    "        tokens.append(next_token.item())\n",
    "        token_emb = model.tok_embeddings(next_token.unsqueeze(0))\n",
    "        input_embedding = torch.cat([input_embedding,token_emb],dim=0)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d043520-913a-40f1-a392-1b432196b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset_path = '../data/videos/msrvtt/'\n",
    "test_file = os.path.join(dataset_path,'test_videodatainfo.json')\n",
    "\n",
    "with open(test_file,'r') as f:\n",
    "    test_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6a2f83d-843d-4983-9a4e-cb1eeb6e9227",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_gt = {}\n",
    "annotations_gt['images'] = test_info['videos']\n",
    "\n",
    "for sentence in test_info['sentences']:\n",
    "    sentence['image_id'] = int(sentence['video_id'].replace('video',''))\n",
    "    sentence['id'] = sentence['video_id']\n",
    "annotations_gt['annotations'] =test_info['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b766fa8-7843-4627-b8fd-5ebaebef8f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "def sample_images_from_video(video_path, num_samples=10):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Calculate total duration in seconds\n",
    "    total_duration = total_frames / frame_rate\n",
    "    # print(total_duration)\n",
    "    # Check if the video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file.\")\n",
    "        return []\n",
    "\n",
    "    # Calculate the interval for sampling\n",
    "    interval = total_frames // num_samples\n",
    "\n",
    "    # Initialize a list to store the sampled images\n",
    "    sampled_images = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Set the frame position\n",
    "        frame_id = i * interval\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "\n",
    "        # Read the frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If frame reading was successful, save the frame\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame)\n",
    "            sampled_images.append(pil_image)\n",
    "            \n",
    "        else:\n",
    "            print(f\"Error reading frame at position {frame_id}\")\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    return sampled_images, total_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8f3cd5d-6b6a-4b75-8e01-3502967c71a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d2bf58b-c2f0-4bda-9c55-d25b53d1863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load(\"ViT-L/14\")\n",
    "clip_model.eval()\n",
    "clip_model = clip_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8012c00-b980-4cea-8eb2-5d5f4624a14b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05527846-984b-43a5-8136-15f5267ce783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c299f86f-e4ee-404d-8ad4-32b625e33c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "annotation_file = os.path.join(dataset_path,'annotation_file')\n",
    "with open(annotation_file,'w') as f:\n",
    "    json.dump(annotations_gt,f)\n",
    "coco = COCO(annotation_file)\n",
    "video_ids = coco.getImgIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6898eb1a-7233-46f3-8060-fea7fd076a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Instruction: Predict the answer based on the video and question.\\nVideo:\"\n",
    "# prompt2 = \"\\nQuestion: Summarize the video.\\nAnswer: It is a video showing\"  #26.7\n",
    "\n",
    "prompt = \"Instruction: Predict the answer based on the video and question.\\nVideo:\"\n",
    "prompt2 = \"\\nQuestion: Can you describe this video?\\nAnswer: It is a video of\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77f161-4b9b-4079-b585-595b27900be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 151/2990 [02:28<36:09,  1.31it/s] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "results = []\n",
    "    \n",
    "for video_id in tqdm(video_ids[:]):\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            video_path = os.path.join(dataset_path,'TestVideo','video'+str(video_id)+'.mp4')\n",
    "            sampled_images,_ = sample_images_from_video(video_path)\n",
    "\n",
    "            image_features = [preprocess(image) for image in sampled_images]\n",
    "            image_features = torch.stack(image_features,dim=0).cuda()\n",
    "            image_features = clip_model.encode_image(image_features)\n",
    "            image_features/=image_features.norm(dim=-1,keepdim=True)\n",
    "            tokens = decoding(model,tokenizer,prompt,prompt2,image_features)\n",
    "            generate_text = tokenizer.decode(tokens[:])\n",
    "            generate_text = generate_text.split('It is a video of')[1].strip().split('.')[0]\n",
    "            results.append({'image_id':video_id,'caption': generate_text})\n",
    "        except:\n",
    "            results.append({'image_id':video_id,'caption': 'A video'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b0e4f-462e-41c1-9fcc-4a59630f0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_result = coco.loadRes(results) \n",
    "\n",
    "coco_eval = COCOEvalCap(coco, coco_result)\n",
    "coco_eval.evaluate()\n",
    "# print output evaluation s|cores\n",
    "scores = {}\n",
    "for metric, score in coco_eval.eval.items():\n",
    "    print(f\"{metric}: {score:.3f}\")\n",
    "    scores[metric] = score\n",
    "with open('13B_msrvtt_results.json','w') as f:\n",
    "    json.dump(scores,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8bf4e-8ba2-4565-b080-feec9bf406c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
