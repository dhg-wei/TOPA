{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebdbbd25-038d-4987-ac84-d5adbdcd40ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "460fe558-fca6-4906-93f3-facd215d9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_arguments(args, filepath):\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(vars(args), file)\n",
    "\n",
    "def load_arguments(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        args_dict = json.load(file)\n",
    "    return args_dict\n",
    "\n",
    "# Optionally, repopulate argparse.ArgumentParser with these arguments\n",
    "def repopulate_arguments(args_dict):\n",
    "    parser = argparse.ArgumentParser(description=\"Example script\")\n",
    "    for key, value in args_dict.items():\n",
    "        parser.add_argument(f'--{key}', type=type(value),default=value)\n",
    "    return parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9c50017-a3cf-4fa3-8a7f-2e24c501a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../vqa_checkpoint/checkpoint_pretrain/llama2_7b_acc4_br5e3_correct_vnips'\n",
    "\n",
    "loaded_args = load_arguments(path+'/args.json')\n",
    "\n",
    "args = repopulate_arguments(loaded_args)\n",
    "args.llama_model_path = '.' +args.llama_model_path\n",
    "args.resume=f'{path}/checkpoint_19.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e884f6-9f56-44a4-aede-7987364a95cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '../vqa_checkpoint/checkpoint_pretrain/llama2_13b_acc8_br8e3_bs4_vnips'\n",
    "\n",
    "# loaded_args = load_arguments(path+'/args.json')\n",
    "\n",
    "# args = repopulate_arguments(loaded_args)\n",
    "# args.llama_model_path = '.' +args.llama_model_path\n",
    "# args.resume='../vqa_checkpoint/checkpoint_pretrain/llama2_13b_acc8_br8e3_bs4_vnips/checkpoint_18.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98596d9e-0760-4eef-995f-7e2d0e47d33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gs/home/lihongyi/anaconda3/envs/llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/gs/home/lihongyi/anaconda3/envs/llama/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from llama import Tokenizer\n",
    "from llama_vqa import LLaMA_VQA\n",
    "from dataloader import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a84fa5-b19f-402d-ac11-08a50a59b126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: 7B\n",
      "loading from ../pretrained/llama2/7B/consolidated.00.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gs/home/lihongyi/anaconda3/envs/llama/lib/python3.11/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model = LLaMA_VQA(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9272202e-d5ed-4645-8411-98463087b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'], strict=False)\n",
    "tokenizer = Tokenizer(model_path=f'{args.llama_model_path}./tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b12971-d47f-4aef-ba77-4d1d163160d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\")\n",
    "clip_model.eval()\n",
    "clip_model = clip_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e125471-d745-4f57-b51a-b311df9de478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "def sample_images_from_video(video_path, num_samples=10):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the total number of frames in the video\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Calculate total duration in seconds\n",
    "    total_duration = total_frames / frame_rate\n",
    "    # print(total_duration)\n",
    "    # Check if the video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file.\")\n",
    "        return []\n",
    "\n",
    "    # Calculate the interval for sampling\n",
    "    interval = total_frames // num_samples\n",
    "\n",
    "    # Initialize a list to store the sampled images\n",
    "    sampled_images = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Set the frame position\n",
    "        frame_id = i * interval\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "\n",
    "        # Read the frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If frame reading was successful, save the frame\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame)\n",
    "            sampled_images.append(pil_image)\n",
    "            \n",
    "        else:\n",
    "            print(f\"Error reading frame at position {frame_id}\")\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    return sampled_images, total_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80ba3180-0ce1-4f7c-817f-79aa02c0ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(model, tokenizer, prompt1,prompt2,video=None):\n",
    "    adapter = model.adapter_query.weight.reshape(-1, model.adapter_len, model.params.dim).unsqueeze(1)\n",
    "    freqs= model.freqs_cis.cuda()\n",
    "    \n",
    "    tokens = [tokenizer.bos_id] + tokenizer.sp_model.encode(prompt1)\n",
    "    query = torch.tensor(tokens, dtype=torch.int64).cuda()\n",
    "    input_embedding = model.tok_embeddings(query)\n",
    "\n",
    "    tokens_2 = tokenizer.sp_model.encode(prompt2)\n",
    "    query_2 = torch.tensor(tokens_2, dtype=torch.int64).cuda()\n",
    "    input_embedding_2 = model.tok_embeddings(query_2)\n",
    "    tokens.extend(tokens_2)\n",
    "    video = video.cuda().float()\n",
    "    video/=video.norm(dim=-1,keepdim=True)\n",
    "    if False:\n",
    "        sim = video@model.memory.T\n",
    "\n",
    "        sim = (sim*100).softmax(dim=-1)\n",
    "\n",
    "        video = sim@model.memory\n",
    "        video = video/video.norm(dim=-1,keepdim=True)\n",
    "        \n",
    "    video_feature = model.visual_proj(video)\n",
    "    video_feature = (video_feature + model.temporal_emb.weight[:, :]).type(model.llamatype)\n",
    "    vqa_video_start=input_embedding.shape[0]\n",
    "    # print(video_feature.shape)\n",
    "    input_embedding = torch.cat([input_embedding,video_feature,input_embedding_2])\n",
    "    start_pos=0\n",
    "    for j in range(15):\n",
    "        vqa_h = input_embedding.unsqueeze(0)\n",
    "        seqlen = vqa_h.shape[-2]\n",
    "        freqs_cis = freqs[:seqlen]\n",
    "        mask = None\n",
    "        mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=vqa_h.device)\n",
    "        mask = torch.triu(mask, diagonal=0 + 1).type_as(vqa_h)\n",
    "\n",
    "        for i, layer in enumerate(model.layers[-1 * model.adapter_layer:]):\n",
    "            vqa_h = layer(vqa_h, start_pos, freqs_cis, mask, adapter[i].type(model.llamatype), vqa_video_start)\n",
    "        vqa_h = model.norm(vqa_h)\n",
    "        vqa_output = model.output(vqa_h)\n",
    "        vqa_output = vqa_output.reshape(-1, model.vocab_size)\n",
    "        next_token = vqa_output[-1,:].argmax()\n",
    "        tokens.append(next_token.item())\n",
    "        token_emb = model.tok_embeddings(next_token.unsqueeze(0))\n",
    "        input_embedding = torch.cat([input_embedding,token_emb],dim=0)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d043520-913a-40f1-a392-1b432196b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../../data/videos/vatex/'\n",
    "test_file = os.path.join(dataset_path,'vatex_validation_v1.0.json')\n",
    "\n",
    "with open(test_file,'r') as f:\n",
    "    test_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67776efb-0079-4b64-a3b9-d122df33eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "for v in test_info:\n",
    "    for caption in v['enCap']:\n",
    "        ann={}\n",
    "        ann['image_id'] = v['videoID']\n",
    "        ann['id'] = v['videoID']\n",
    "        ann['caption'] = caption\n",
    "        annotations.append(ann)\n",
    "annotations_gt = {}\n",
    "annotations_gt['images'] = [{'id':v['videoID'],'videoID':v['videoID']} for v in test_info]\n",
    "annotations_gt['annotations'] =annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6a2f83d-843d-4983-9a4e-cb1eeb6e9227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')\n",
    "import sys\n",
    "\n",
    "annotation_file = os.path.join(dataset_path,'annotation_file')\n",
    "with open(annotation_file,'w') as f:\n",
    "    json.dump(annotations_gt,f)\n",
    "coco = COCO(annotation_file)\n",
    "video_ids = coco.getImgIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3cd5d-6b6a-4b75-8e01-3502967c71a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2bf58b-c2f0-4bda-9c55-d25b53d1863d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6898eb1a-7233-46f3-8060-fea7fd076a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Instruction: Predict the answer based on the video and question.\\nVideo:\"\n",
    "# prompt2 = \"\\nQuestion: Summarize the video.\\nAnswer: It is a video showing\"  #26.7\n",
    "\n",
    "prompt = \"Instruction: Generate a dense description for the video.\\nVideo:\"\n",
    "prompt2 = \"\\nVideo Caption: The video shows\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af77f161-4b9b-4079-b585-595b27900be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "results = []\n",
    "    \n",
    "for video_id in tqdm(video_ids[:10]):\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "                \n",
    "            video_path = os.path.join(dataset_path,'val_all',str(video_id)+'.mp4')\n",
    "            sampled_images,_ = sample_images_from_video(video_path)\n",
    "    \n",
    "            image_features = [preprocess(image) for image in sampled_images]\n",
    "            image_features = torch.stack(image_features,dim=0).cuda()\n",
    "            image_features = clip_model.encode_image(image_features)\n",
    "            image_features/=image_features.norm(dim=-1,keepdim=True)\n",
    "            tokens = decoding(model,tokenizer,prompt,prompt2,image_features)\n",
    "            generate_text = tokenizer.decode(tokens[:])\n",
    "            generate_text = generate_text.split('Video Caption: ')[1].replace(\"The video shows\",'').strip().split('.')[0]\n",
    "            results.append({'image_id':video_id,'caption': generate_text})\n",
    "        except:\n",
    "            results.append({'image_id':video_id,'caption': 'A video'})\n",
    "# for video_id in video_ids[10:]:\n",
    "#     results.append({'image_id':video_id,'caption': 'a video'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2faaab6c-525b-4acc-a41c-80322849fcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': 'G9zN5TTuGO4_000179_000189',\n",
       "  'caption': 'a man named Igor Kuzmich climbing a snowy mountain'},\n",
       " {'image_id': 'CQzUU7-cVck_000006_000016',\n",
       "  'caption': 'a man jumping from a tree to a wooden obstacle course'},\n",
       " {'image_id': 'OMK0OJ4f_TI_000000_000010',\n",
       "  'caption': 'a man wearing a black hood and a black mask, and he'},\n",
       " {'image_id': 'xnyOA58A07Q_000127_000137',\n",
       "  'caption': 'Beth and Jim Acosta, two climbers, rappelling down a cl'},\n",
       " {'image_id': 'n6lUXDwL4Y0_000022_000032',\n",
       "  'caption': 'a man demonstrating the importance of being flexible and having a positive attitude when'},\n",
       " {'image_id': 'cU1qVk7HXfE_000304_000314',\n",
       "  'caption': 'a man in a Jesus Christ costume performing a striptease'},\n",
       " {'image_id': '7CN3ENwfMBE_000096_000106',\n",
       "  'caption': 'a man performing a one-man show about the character Ostroff from'},\n",
       " {'image_id': 'rW0KwHhQZTE_000013_000023',\n",
       "  'caption': 'a group of students performing an act of mischief'},\n",
       " {'image_id': 'KKm1bM51CZs_000070_000080',\n",
       "  'caption': 'a man and a woman performing a play'},\n",
       " {'image_id': 'gSxbTg_EGz4_000046_000056',\n",
       "  'caption': 'a man who is playing the role of a father'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "602b0e4f-462e-41c1-9fcc-4a59630f0c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 486507 tokens at 1215912.69 tokens per second.\n",
      "PTBTokenizer tokenized 28554 tokens at 393917.56 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 25107, 'reflen': 32132, 'guess': [25107, 22107, 19107, 16107], 'correct': [17572, 6785, 2294, 735]}\n",
      "ratio: 0.7813705962902782\n",
      "Bleu_1: 0.529\n",
      "Bleu_2: 0.350\n",
      "Bleu_3: 0.223\n",
      "Bleu_4: 0.140\n",
      "computing METEOR score...\n",
      "METEOR: 0.166\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.371\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.310\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.7 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.3 sec].\n",
      "Threads( StanfordCoreNLP ) [5.362 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 15.56 s\n",
      "SPICE: 0.074\n",
      "Bleu_1: 0.529\n",
      "Bleu_2: 0.350\n",
      "Bleu_3: 0.223\n",
      "Bleu_4: 0.140\n",
      "METEOR: 0.166\n",
      "ROUGE_L: 0.371\n",
      "CIDEr: 0.310\n",
      "SPICE: 0.074\n"
     ]
    }
   ],
   "source": [
    "coco_result = coco.loadRes(results) \n",
    "\n",
    "coco_eval = COCOEvalCap(coco, coco_result)\n",
    "coco_eval.evaluate()\n",
    "# print output evaluation s|cores\n",
    "scores = {}\n",
    "for metric, score in coco_eval.eval.items():\n",
    "    print(f\"{metric}: {score:.3f}\")\n",
    "    scores[metric] = score\n",
    "with open('vatex_7B_results.json','w') as f:\n",
    "    json.dump(scores,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcf94388-e017-43e5-911b-3e88d35167ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 617049 tokens at 947671.93 tokens per second.\n",
      "PTBTokenizer tokenized 23044 tokens at 341563.64 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 20039, 'reflen': 21562, 'guess': [20039, 17049, 14060, 11076], 'correct': [14340, 6147, 1952, 667]}\n",
      "ratio: 0.9293664780632163\n",
      "Bleu_1: 0.663\n",
      "Bleu_2: 0.471\n",
      "Bleu_3: 0.306\n",
      "Bleu_4: 0.200\n",
      "computing METEOR score...\n",
      "METEOR: 0.214\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.481\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.307\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 7.328 s\n",
      "SPICE: 0.053\n",
      "Bleu_1: 0.663\n",
      "Bleu_2: 0.471\n",
      "Bleu_3: 0.306\n",
      "Bleu_4: 0.200\n",
      "METEOR: 0.214\n",
      "ROUGE_L: 0.481\n",
      "CIDEr: 0.307\n",
      "SPICE: 0.053\n"
     ]
    }
   ],
   "source": [
    "coco_result = coco.loadRes(results) \n",
    "\n",
    "coco_eval = COCOEvalCap(coco, coco_result)\n",
    "coco_eval.evaluate()\n",
    "# print output evaluation scores\n",
    "scores = {}\n",
    "for metric, score in coco_eval.eval.items():\n",
    "    print(f\"{metric}: {score:.3f}\")\n",
    "    scores['metric'] = score\n",
    "with open('msrvtt_results.json','w') as f:\n",
    "    json.dump(scores,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28a28e6-ca3e-46a8-abda-72581605eff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
