{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gs/home/lihongyi/anaconda3/envs/llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import Video, HTML\n",
    "\n",
    "from decord import VideoReader, cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_arguments(args, filepath):\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(vars(args), file)\n",
    "\n",
    "def load_arguments(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        args_dict = json.load(file)\n",
    "    return args_dict\n",
    "\n",
    "# Optionally, repopulate argparse.ArgumentParser with these arguments\n",
    "def repopulate_arguments(args_dict):\n",
    "    parser = argparse.ArgumentParser(description=\"Example script\")\n",
    "    for key, value in args_dict.items():\n",
    "        parser.add_argument(f'--{key}', type=type(value),default=value)\n",
    "    return parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: 13B\n",
      "loading from ../pretrained/llama2/13B/consolidated.00.pth\n",
      "loading from ../pretrained/llama2/13B/consolidated.01.pth\n",
      "gathering layer 0 of 40\n",
      "gathering layer 1 of 40\n",
      "gathering layer 2 of 40\n",
      "gathering layer 3 of 40\n",
      "gathering layer 4 of 40\n",
      "gathering layer 5 of 40\n",
      "gathering layer 6 of 40\n",
      "gathering layer 7 of 40\n",
      "gathering layer 8 of 40\n",
      "gathering layer 9 of 40\n",
      "gathering layer 10 of 40\n",
      "gathering layer 11 of 40\n",
      "gathering layer 12 of 40\n",
      "gathering layer 13 of 40\n",
      "gathering layer 14 of 40\n",
      "gathering layer 15 of 40\n",
      "gathering layer 16 of 40\n",
      "gathering layer 17 of 40\n",
      "gathering layer 18 of 40\n",
      "gathering layer 19 of 40\n",
      "gathering layer 20 of 40\n",
      "gathering layer 21 of 40\n",
      "gathering layer 22 of 40\n",
      "gathering layer 23 of 40\n",
      "gathering layer 24 of 40\n",
      "gathering layer 25 of 40\n",
      "gathering layer 26 of 40\n",
      "gathering layer 27 of 40\n",
      "gathering layer 28 of 40\n",
      "gathering layer 29 of 40\n",
      "gathering layer 30 of 40\n",
      "gathering layer 31 of 40\n",
      "gathering layer 32 of 40\n",
      "gathering layer 33 of 40\n",
      "gathering layer 34 of 40\n",
      "gathering layer 35 of 40\n",
      "gathering layer 36 of 40\n",
      "gathering layer 37 of 40\n",
      "gathering layer 38 of 40\n",
      "gathering layer 39 of 40\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacty of 39.59 GiB of which 2.19 MiB is free. Including non-PyTorch memory, this process has 39.58 GiB memory in use. Of the allocated memory 38.47 GiB is allocated by PyTorch, and 133.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mLLaMA_VQA\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(args\u001b[38;5;241m.\u001b[39mresume, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m], strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/gsdata/home/lihongyi/nips/TOPA/demos/../llama_vqa.py:68\u001b[0m, in \u001b[0;36mLLaMA_VQA\u001b[0;34m(args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     model_llama_vqa \u001b[38;5;241m=\u001b[39m Transformer_llama3(model_args, args)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     model_llama_vqa \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_default_tensor_type(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\n\u001b[1;32m     70\u001b[0m missing_keys, unexpected_keys \u001b[38;5;241m=\u001b[39m model_llama_vqa\u001b[38;5;241m.\u001b[39mload_state_dict(full_state_dict, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/gsdata/home/lihongyi/nips/TOPA/demos/../llama/model.py:205\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, params, args)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(params\u001b[38;5;241m.\u001b[39mdim, eps\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mnorm_eps)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m Linear(params\u001b[38;5;241m.\u001b[39mdim, params\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/gsdata/home/lihongyi/nips/TOPA/demos/../llama/model.py:154\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[0;34m(self, layer_id, args)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdim\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m args\u001b[38;5;241m.\u001b[39mn_heads\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m \u001b[43mAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward \u001b[38;5;241m=\u001b[39m FeedForward(dim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdim, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mdim, multiple_of\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmultiple_of)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_id \u001b[38;5;241m=\u001b[39m layer_id\n",
      "File \u001b[0;32m/gsdata/home/lihongyi/nips/TOPA/demos/../llama/model.py:86\u001b[0m, in \u001b[0;36mAttention.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwo \u001b[38;5;241m=\u001b[39m Linear(args\u001b[38;5;241m.\u001b[39mn_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, args\u001b[38;5;241m.\u001b[39mdim, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((args\u001b[38;5;241m.\u001b[39mmax_batch_size, args\u001b[38;5;241m.\u001b[39mmax_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_local_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim))\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_v \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_local_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_local_heads, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_local_heads, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39margs\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacty of 39.59 GiB of which 2.19 MiB is free. Including non-PyTorch memory, this process has 39.58 GiB memory in use. Of the allocated memory 38.47 GiB is allocated by PyTorch, and 133.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "path = '../vqa_checkpoint/checkpoint_pretrain/llama2_13b_acc8_br8e3_bs4_vnips'\n",
    "\n",
    "loaded_args = load_arguments(path+'/args.json')\n",
    "\n",
    "args = repopulate_arguments(loaded_args)\n",
    "args.llama_model_path = '.' +args.llama_model_path\n",
    "args.resume='../vqa_checkpoint/checkpoint_pretrain/llama2_13b_acc8_br8e3_bs4_vnips/checkpoint_18.pth'\n",
    "\n",
    "\n",
    "# path = '../vqa_checkpoint/checkpoint_pretrain/llama2_7b_acc4_br5e3_correct_vnips'\n",
    "\n",
    "# loaded_args = load_arguments(path+'/args.json')\n",
    "\n",
    "# args = repopulate_arguments(loaded_args)\n",
    "# args.llama_model_path = '.' +args.llama_model_path\n",
    "# args.resume='../../vqa_checkpoint/checkpoint_pretrain/llama2_7b_acc4_br5e3_correct_vnips/checkpoint_19.pth'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from llama import Tokenizer\n",
    "from llama_vqa import LLaMA_VQA\n",
    "from dataloader import load_data\n",
    "\n",
    "with torch.no_grad():\n",
    "    model = LLaMA_VQA(args)\n",
    "\n",
    "checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'], strict=False)\n",
    "tokenizer = Tokenizer(model_path=f'{args.llama_model_path}./tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.resume='../../vqa_checkpoint/checkpoint_finetune/tag_vnip_llama2_13b_finetune_nextqa_2e3_acc16/checkpoint_4.pth'\n",
    "# args.resume='../../vqa_checkpoint/checkpoint_finetune/tag_vnip_llama2_13b_finetune_star_2e3_acc16/checkpoint_1.pth'\n",
    "# args.resume='../../vqa_checkpoint/checkpoint_pretrain/llama2_13b_acc8_br8e3_bs4_vnips/checkpoint_18.pth'\n",
    "# args.llama_model_path = '.' +args.llama_model_path\n",
    "# checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "# model.load_state_dict(checkpoint['model'], strict=False)\n",
    "# tokenizer = Tokenizer(model_path=f'{args.llama_model_path}./tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutli_choice_decoding(model, tokenizer, prompt1,prompt2,video=None):\n",
    "    adapter = model.adapter_query.weight.reshape(-1, model.adapter_len, model.params.dim).unsqueeze(1)\n",
    "    freqs= model.freqs_cis.cuda()\n",
    "    \n",
    "    tokens = [tokenizer.bos_id] + tokenizer.sp_model.encode(prompt1)\n",
    "    query = torch.tensor(tokens, dtype=torch.int64).cuda()\n",
    "    input_embedding = model.tok_embeddings(query)\n",
    "\n",
    "    tokens_2 = tokenizer.sp_model.encode(prompt2)\n",
    "    query_2 = torch.tensor(tokens_2, dtype=torch.int64).cuda()\n",
    "    input_embedding_2 = model.tok_embeddings(query_2)\n",
    "    tokens.extend(tokens_2)\n",
    "    video = video.cuda().float()\n",
    "    video/=video.norm(dim=-1,keepdim=True)\n",
    "    if True:\n",
    "        sim = video@model.memory.T\n",
    "\n",
    "        sim = (sim*100).softmax(dim=-1)\n",
    "\n",
    "        video = sim@model.memory\n",
    "        video = video/video.norm(dim=-1,keepdim=True)\n",
    "        \n",
    "    video_feature = model.visual_proj(video)\n",
    "    video_feature = (video_feature + model.temporal_emb.weight[:, :]).type(model.llamatype)\n",
    "    vqa_video_start=input_embedding.shape[0]\n",
    "    # print(video_feature.shape)\n",
    "    input_embedding = torch.cat([input_embedding,video_feature,input_embedding_2])\n",
    "    start_pos=0\n",
    "    for j in range(2):\n",
    "        vqa_h = input_embedding.unsqueeze(0)\n",
    "        seqlen = vqa_h.shape[-2]\n",
    "        freqs_cis = freqs[:seqlen]\n",
    "        mask = None\n",
    "        mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=vqa_h.device)\n",
    "        mask = torch.triu(mask, diagonal=0 + 1).type_as(vqa_h)\n",
    "\n",
    "        for i, layer in enumerate(model.layers[-1 * model.adapter_layer:]):\n",
    "            vqa_h = layer(vqa_h, start_pos, freqs_cis, mask, adapter[i].type(model.llamatype), vqa_video_start)\n",
    "        vqa_h = model.norm(vqa_h)\n",
    "        vqa_output = model.output(vqa_h)\n",
    "        vqa_output = vqa_output.reshape(-1, model.vocab_size)\n",
    "        vqa_output[-1,920]=-100\n",
    "        vqa_output[-1,1128]=-100\n",
    "        next_token = vqa_output[-1,:].argmax()\n",
    "        tokens.append(next_token.item())\n",
    "        token_emb = model.tok_embeddings(next_token.unsqueeze(0))\n",
    "        input_embedding = torch.cat([input_embedding,token_emb],dim=0)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_data_path = \"../../mvbench/MVBench/video\"\n",
    "data_list = {\n",
    "    \"Action Sequence\": (\"action_sequence.json\", f\"{your_data_path}/star/Charades_v1_480/\", \"video\", True), # has start & end\n",
    "    \"Action Prediction\": (\"action_prediction.json\", f\"{your_data_path}/star/Charades_v1_480/\", \"video\", True), # has start & end\n",
    "    \"Action Antonym\": (\"action_antonym.json\", f\"{your_data_path}/ssv2_video/\", \"video\", False),\n",
    "    \"Fine-grained Action\": (\"fine_grained_action.json\", f\"{your_data_path}/Moments_in_Time_Raw/videos/\", \"video\", False),\n",
    "    \"Unexpected Action\": (\"unexpected_action.json\", f\"{your_data_path}/FunQA_test/test/\", \"video\", False),\n",
    "    \"Object Existence\": (\"object_existence.json\", f\"{your_data_path}/clevrer/video_validation/\", \"video\", False),\n",
    "    \"Object Interaction\": (\"object_interaction.json\", f\"{your_data_path}/star/Charades_v1_480/\", \"video\", True), # has start & end\n",
    "    \"Object Shuffle\": (\"object_shuffle.json\", f\"{your_data_path}/perception/videos/\", \"video\", False),\n",
    "    \"Moving Direction\": (\"moving_direction.json\", f\"{your_data_path}/clevrer/video_validation/\", \"video\", False),\n",
    "    \"Action Localization\": (\"action_localization.json\", f\"{your_data_path}/sta/sta_video/\", \"video\", True),  # has start & end\n",
    "    \"Scene Transition\": (\"scene_transition.json\", f\"{your_data_path}/scene_qa/video/\", \"video\", False),\n",
    "    \"Action Count\": (\"action_count.json\", f\"{your_data_path}/perception/videos/\", \"video\", False),\n",
    "    \"Moving Count\": (\"moving_count.json\", f\"{your_data_path}/clevrer/video_validation/\", \"video\", False),\n",
    "    \"Moving Attribute\": (\"moving_attribute.json\", f\"{your_data_path}/clevrer/video_validation/\", \"video\", False),\n",
    "    \"State Change\": (\"state_change.json\", f\"{your_data_path}/perception/videos/\", \"video\", False),\n",
    "    \"Fine-grained Pose\": (\"fine_grained_pose.json\", f\"{your_data_path}/nturgbd/\", \"video\", False),\n",
    "    \"Character Order\": (\"character_order.json\", f\"{your_data_path}/perception/videos/\", \"video\", False),\n",
    "    \"Egocentric Navigation\": (\"egocentric_navigation.json\", f\"{your_data_path}/vlnqa/\", \"video\", False),\n",
    "    \"Episodic Reasoning\": (\"episodic_reasoning.json\", f\"{your_data_path}/tvqa/frames_fps3_hq/\", \"frame\", True),  # has start & end, read frame\n",
    "    \"Counterfactual Inference\": (\"counterfactual_inference.json\", f\"{your_data_path}/clevrer/video_validation/\", \"video\", False),\n",
    "}\n",
    "\n",
    "data_dir = \"../../mvbench/MVBench/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model=clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from video_transforms import (\n",
    "    GroupNormalize, GroupScale, GroupCenterCrop, \n",
    "    Stack, ToTorchFormatTensor\n",
    ")\n",
    "\n",
    "class MVBench_dataset(Dataset):\n",
    "    def __init__(self, data_dir, data_list, num_segments=8, resolution=224):\n",
    "        self.data_list = []\n",
    "        for k, v in data_list.items():\n",
    "            with open(os.path.join(data_dir, v[0]), 'r') as f:\n",
    "                json_data = json.load(f)\n",
    "            for data in json_data:\n",
    "                self.data_list.append({\n",
    "                    'task_type': k,\n",
    "                    'prefix': v[1],\n",
    "                    'data_type': v[2],\n",
    "                    'bound': v[3],\n",
    "                    'data': data\n",
    "                })\n",
    "        \n",
    "        self.decord_method = {\n",
    "            'video': self.read_video,\n",
    "            'gif': self.read_gif,\n",
    "            'frame': self.read_frame,\n",
    "        }\n",
    "        \n",
    "        self.num_segments = num_segments\n",
    "        \n",
    "        # transform\n",
    "        crop_size = resolution\n",
    "        scale_size = resolution\n",
    "        input_mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "        input_std = [0.26862954, 0.26130258, 0.27577711]\n",
    "        self.transform = preprocess\n",
    "    \n",
    "    def __str__(self):\n",
    "        len_list = {}\n",
    "        option_list = {}\n",
    "        for data in self.data_list:\n",
    "            if data['task_type'] not in len_list:\n",
    "                len_list[data['task_type']] = 0\n",
    "            len_list[data['task_type']] += 1\n",
    "            if data['task_type'] not in option_list:\n",
    "                option_list[data['task_type']] = 0\n",
    "            option_list[data['task_type']] += len(data['data']['candidates'])\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        res = f\"There are {len(self.data_list)} videos as follow:\\n\"\n",
    "        for k, v in len_list.items():\n",
    "            correct += len_list[k]\n",
    "            total += option_list[k]\n",
    "            res += f\"{v} for {k} ({option_list[k]} options => {len_list[k]/option_list[k]*100:.2f}%)\\n\"\n",
    "            correct = correct + 1 / option_list[k]\n",
    "        res += f\"Total random accuracy: {correct/total*100:.2f}%\"\n",
    "        return res.rstrip()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def get_index(self, bound, fps, max_frame, first_idx=0):\n",
    "        if bound:\n",
    "            start, end = bound[0], bound[1]\n",
    "        else:\n",
    "            start, end = -100000, 100000\n",
    "        start_idx = max(first_idx, round(start * fps))\n",
    "        end_idx = min(round(end * fps), max_frame)\n",
    "        seg_size = float(end_idx - start_idx) / self.num_segments\n",
    "        frame_indices = np.array([\n",
    "            int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n",
    "            for idx in range(self.num_segments)\n",
    "        ])\n",
    "        return frame_indices\n",
    "    \n",
    "    def read_video(self, video_path, bound=None):\n",
    "        # cap = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        max_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        # vr = cv2.VideoCapture(video_path)\n",
    "        # max_frame = len(vr) - 1\n",
    "        # fps = float(vr.get_avg_fps())\n",
    "    \n",
    "        images_group = list()\n",
    "        frame_indices = self.get_index(bound, fps, max_frame, first_idx=0) \n",
    "\n",
    "        for frame_index in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame)\n",
    "\n",
    "            images_group.append(pil_image)\n",
    "\n",
    "\n",
    "        # print(torch_imgs.shape)\n",
    "        return images_group\n",
    "\n",
    "    def read_gif(self, video_path, bound=None, fps=25):\n",
    "        gif = imageio.get_reader(video_path)\n",
    "        max_frame = len(gif) - 1\n",
    "        \n",
    "        images_group = list()\n",
    "        frame_indices = self.get_index(bound, fps, max_frame, first_idx=0) \n",
    "        for index, frame in enumerate(gif):\n",
    "            if index in frame_indices:\n",
    "                img = cv2.cvtColor(frame, cv2.COLOR_RGBA2RGB)\n",
    "                img = Image.fromarray(img)\n",
    "                images_group.append(img)\n",
    "\n",
    "        # print(torch_imgs.shape)\n",
    "        return images_group\n",
    "    \n",
    "    def read_frame(self, video_path, bound=None, fps=3):\n",
    "        max_frame = len(os.listdir(video_path))\n",
    "        images_group = list()\n",
    "        frame_indices = self.get_index(bound, fps, max_frame, first_idx=1) # frame_idx starts from 1\n",
    "        for frame_index in frame_indices:\n",
    "            img = Image.open(os.path.join(video_path, f\"{frame_index:05d}.jpg\"))\n",
    "            images_group.append(img)\n",
    "        # print(torch_imgs.shape)\n",
    "        return images_group\n",
    "\n",
    "    def qa_template(self, data):\n",
    "        question = f\"Question: {data['question']}\\n\"\n",
    "        question += \"Choices: \\n\"\n",
    "        answer = data['answer']\n",
    "        answer_idx = -1\n",
    "        for idx, c in enumerate(data['candidates']):\n",
    "            question += f\"({chr(ord('A') + idx)}) {c}\\n\"\n",
    "            if c == answer:\n",
    "                answer_idx = idx\n",
    "        question = question\n",
    "        answer = f\"({chr(ord('A') + answer_idx)}) {answer}\"\n",
    "        return question, answer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        decord_method = self.decord_method[self.data_list[idx]['data_type']]\n",
    "        bound = None\n",
    "        if self.data_list[idx]['bound']:\n",
    "            bound = (\n",
    "                self.data_list[idx]['data']['start'],\n",
    "                self.data_list[idx]['data']['end'],\n",
    "            )\n",
    "        video_path = os.path.join(self.data_list[idx]['prefix'], self.data_list[idx]['data']['video'])\n",
    "        question, answer = self.qa_template(self.data_list[idx]['data'])\n",
    "        try:\n",
    "            torch_imgs = decord_method(video_path, bound)\n",
    "        except:\n",
    "            return {\n",
    "                'video': None, \n",
    "                'question': question, \n",
    "                'answer': answer,\n",
    "                'task_type': self.data_list[idx]['task_type']\n",
    "            }  \n",
    "        # print(torch_imgs.shape)\n",
    "\n",
    "            \n",
    "        return {\n",
    "            'video': torch_imgs, \n",
    "            'question': question, \n",
    "            'answer': answer,\n",
    "            'task_type': self.data_list[idx]['task_type']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  position embedding\n",
    "num_frame = 10\n",
    "resolution = 224\n",
    "\n",
    "dataset = MVBench_dataset(data_dir, data_list, num_segments=num_frame, resolution=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ans(pred, gt):\n",
    "    flag = False\n",
    "    \n",
    "    pred_list = pred.lower().split(' ')\n",
    "    pred_option, pred_content = pred_list[0], ' '.join(pred_list[1:])\n",
    "    gt_list = gt.lower().split(' ')\n",
    "    gt_option, gt_content = gt_list[0], ' '.join(gt_list[1:])\n",
    "    if gt_content[-1] == '.':\n",
    "        gt_content = gt_content[:-1]\n",
    "    \n",
    "    if pred_option.replace('.', '') in gt_option:\n",
    "        flag = True\n",
    "    elif gt_option in pred_option:\n",
    "        flag = True\n",
    "        \n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 100/4000 [00:58<29:05,  2.23it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 43.00%\n",
      "Total Acc: 43.00%\n",
      "------------------------------ Action Sequence ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 200/4000 [01:42<26:35,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 42.00%\n",
      "Total Acc: 42.00%\n",
      "------------------------------ Action Sequence ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 300/4000 [02:26<24:35,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 39.00%\n",
      "Total Acc: 41.00%\n",
      "------------------------------ Action Prediction ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 400/4000 [03:08<23:53,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 38.50%\n",
      "Total Acc: 40.25%\n",
      "------------------------------ Action Prediction ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 436/4000 [03:19<17:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 462/4000 [03:26<17:15,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 470/4000 [03:28<13:07,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 487/4000 [03:33<13:47,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 491/4000 [03:34<16:39,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 500/4000 [03:36<16:50,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 38.00%\n",
      "Total Acc: 39.80%\n",
      "------------------------------ Action Antonym ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 505/4000 [03:38<16:22,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 533/4000 [03:46<14:23,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 540/4000 [03:48<16:17,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 548/4000 [03:50<18:59,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 561/4000 [03:53<15:23,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 566/4000 [03:55<16:32,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 601/4000 [04:05<16:14,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 35.00%\n",
      "Total Acc: 38.50%\n",
      "------------------------------ Action Antonym ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 700/4000 [04:39<17:26,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 32.00%\n",
      "Total Acc: 37.57%\n",
      "------------------------------ Fine-grained Action ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 800/4000 [05:15<17:20,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 34.50%\n",
      "Total Acc: 37.50%\n",
      "------------------------------ Fine-grained Action ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 900/4000 [06:36<1:15:20,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 66.00%\n",
      "Total Acc: 40.67%\n",
      "------------------------------ Unexpected Action ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1000/4000 [08:19<42:21,  1.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 66.00%\n",
      "Total Acc: 43.20%\n",
      "------------------------------ Unexpected Action ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1100/4000 [09:12<26:35,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 52.00%\n",
      "Total Acc: 44.00%\n",
      "------------------------------ Object Existence ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1200/4000 [10:05<26:22,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 52.50%\n",
      "Total Acc: 44.75%\n",
      "------------------------------ Object Existence ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 1300/4000 [10:49<18:38,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 47.00%\n",
      "Total Acc: 44.92%\n",
      "------------------------------ Object Interaction ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 1385/4000 [11:27<18:27,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 1400/4000 [11:32<17:48,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 47.50%\n",
      "Total Acc: 45.14%\n",
      "------------------------------ Object Interaction ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1500/4000 [14:16<56:23,  1.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 30.00%\n",
      "Total Acc: 44.13%\n",
      "------------------------------ Object Shuffle ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1600/4000 [17:04<1:12:40,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 28.00%\n",
      "Total Acc: 43.00%\n",
      "------------------------------ Object Shuffle ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 1700/4000 [17:56<19:50,  1.93it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 21.00%\n",
      "Total Acc: 41.71%\n",
      "------------------------------ Moving Direction ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 1800/4000 [18:50<20:59,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 22.00%\n",
      "Total Acc: 40.67%\n",
      "------------------------------ Moving Direction ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1900/4000 [20:24<28:43,  1.22it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 54.00%\n",
      "Total Acc: 41.37%\n",
      "------------------------------ Action Localization ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2000/4000 [21:49<29:29,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 37.50%\n",
      "Total Acc: 40.35%\n",
      "------------------------------ Action Localization ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 2100/4000 [22:15<08:10,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 80.00%\n",
      "Total Acc: 42.24%\n",
      "------------------------------ Scene Transition ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 2200/4000 [22:41<07:59,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 81.00%\n",
      "Total Acc: 44.05%\n",
      "------------------------------ Scene Transition ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 2300/4000 [25:16<33:38,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 34.00%\n",
      "Total Acc: 43.61%\n",
      "------------------------------ Action Count ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 2400/4000 [27:48<33:16,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 38.00%\n",
      "Total Acc: 43.54%\n",
      "------------------------------ Action Count ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 2500/4000 [28:42<13:20,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 26.00%\n",
      "Total Acc: 42.84%\n",
      "------------------------------ Moving Count ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 2600/4000 [29:35<13:22,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 24.00%\n",
      "Total Acc: 42.04%\n",
      "------------------------------ Moving Count ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 2700/4000 [30:28<11:04,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 41.00%\n",
      "Total Acc: 42.00%\n",
      "------------------------------ Moving Attribute ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 2800/4000 [31:22<10:37,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 42.50%\n",
      "Total Acc: 42.07%\n",
      "------------------------------ Moving Attribute ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 2900/4000 [34:18<34:52,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 49.00%\n",
      "Total Acc: 42.31%\n",
      "------------------------------ State Change ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3000/4000 [37:07<28:54,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 41.50%\n",
      "Total Acc: 42.03%\n",
      "------------------------------ State Change ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 3100/4000 [38:02<08:38,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 33.00%\n",
      "Total Acc: 41.74%\n",
      "------------------------------ Fine-grained Pose ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 3200/4000 [38:56<06:03,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 28.50%\n",
      "Total Acc: 41.19%\n",
      "------------------------------ Fine-grained Pose ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 3300/4000 [41:50<24:09,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 35.00%\n",
      "Total Acc: 41.00%\n",
      "------------------------------ Character Order ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 3400/4000 [44:48<17:46,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 34.00%\n",
      "Total Acc: 40.76%\n",
      "------------------------------ Character Order ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 3500/4000 [45:11<02:06,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 46.00%\n",
      "Total Acc: 40.91%\n",
      "------------------------------ Egocentric Navigation ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 3600/4000 [45:33<01:35,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 23.50%\n",
      "Total Acc: 39.81%\n",
      "------------------------------ Egocentric Navigation ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 3700/4000 [46:03<01:30,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 50.00%\n",
      "Total Acc: 40.08%\n",
      "------------------------------ Episodic Reasoning ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 3800/4000 [46:34<01:04,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 49.00%\n",
      "Total Acc: 40.29%\n",
      "------------------------------ Episodic Reasoning ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 3900/4000 [47:32<00:55,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 32.00%\n",
      "Total Acc: 40.08%\n",
      "------------------------------ Counterfactual Inference ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [48:30<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  Acc: 30.50%\n",
      "Total Acc: 39.80%\n",
      "------------------------------ Counterfactual Inference ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_path = \"./test\"\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "res_list = []\n",
    "acc_dict = {}\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    task_type = example['task_type']\n",
    "    if task_type not in acc_dict:\n",
    "        acc_dict[task_type] = [0, 0] # correct, total\n",
    "    acc_dict[task_type][1] += 1\n",
    "    total += 1\n",
    "\n",
    "    if example['video']==None:\n",
    "        pred='(A)'\n",
    "        print(1)\n",
    "    else:\n",
    "        prompt = \"Instruction: Choose the correct answer based on the video and question.\\nVideo:\"\n",
    "        prompt2 = f\"\\n{example['question']}Answer: The correct choice is (\"\n",
    "    \n",
    "        tokens = mutli_choice_decoding(model,tokenizer,prompt,prompt2,example['video'])\n",
    "        generate_text = tokenizer.decode(tokens[:])\n",
    "        pred = generate_text.split('The correct choice is ')[1]\n",
    "\n",
    "\n",
    "    gt = example['answer']\n",
    "    res_list.append({\n",
    "        'pred': pred,\n",
    "        'gt': gt\n",
    "    })\n",
    "    if check_ans(pred=pred, gt=gt):\n",
    "        acc_dict[task_type][0] += 1\n",
    "        correct += 1\n",
    "    if total%100==0:\n",
    "        print(f\"Part  Acc: {acc_dict[task_type][0] / acc_dict[task_type][1] * 100 :.2f}%\")\n",
    "        print(f\"Total Acc: {correct / total * 100 :.2f}%\")\n",
    "        print('-' * 30, task_type, '-' * 30)\n",
    "    # break\n",
    "\n",
    "with open(f\"{save_path}.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"acc_dict\": acc_dict,\n",
    "        \"res_list\": res_list\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutli_choice_decoding(model, tokenizer, prompt1,prompt2,video=None):\n",
    "    with torch.no_grad():\n",
    "        torch_imgs = [preprocess(image) for image in video]\n",
    "        video = torch.stack(torch_imgs,dim=0)\n",
    "        \n",
    "        video = video.cuda()\n",
    "        video = clip_model.encode_image(video)\n",
    "        video/=video.norm(dim=-1,keepdim=True)\n",
    "        adapter = model.adapter_query.weight.reshape(-1, model.adapter_len, model.params.dim).unsqueeze(1)\n",
    "        freqs= model.freqs_cis.cuda()\n",
    "        \n",
    "        tokens = [tokenizer.bos_id] + tokenizer.sp_model.encode(prompt1)\n",
    "        query = torch.tensor(tokens, dtype=torch.int64).cuda()\n",
    "        input_embedding = model.tok_embeddings(query)\n",
    "    \n",
    "        tokens_2 = tokenizer.sp_model.encode(prompt2)\n",
    "        query_2 = torch.tensor(tokens_2, dtype=torch.int64).cuda()\n",
    "        input_embedding_2 = model.tok_embeddings(query_2)\n",
    "        tokens.extend(tokens_2)\n",
    "        video = video.cuda().float()\n",
    "        video/=video.norm(dim=-1,keepdim=True)\n",
    "        if True:\n",
    "            sim = video@model.memory.T\n",
    "    \n",
    "            sim = (sim*100).softmax(dim=-1)\n",
    "    \n",
    "            video = sim@model.memory\n",
    "            video = video/video.norm(dim=-1,keepdim=True)\n",
    "            \n",
    "        video_feature = model.visual_proj(video)\n",
    "        video_feature = (video_feature + model.temporal_emb.weight[:, :]).type(model.llamatype)\n",
    "        vqa_video_start=input_embedding.shape[0]\n",
    "        # print(video_feature.shape)\n",
    "        input_embedding = torch.cat([input_embedding,video_feature,input_embedding_2])\n",
    "        start_pos=0\n",
    "        for j in range(2):\n",
    "            vqa_h = input_embedding.unsqueeze(0)\n",
    "            seqlen = vqa_h.shape[-2]\n",
    "            freqs_cis = freqs[:seqlen]\n",
    "            mask = None\n",
    "            mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=vqa_h.device)\n",
    "            mask = torch.triu(mask, diagonal=0 + 1).type_as(vqa_h)\n",
    "    \n",
    "            for i, layer in enumerate(model.layers[-1 * model.adapter_layer:]):\n",
    "                vqa_h = layer(vqa_h, start_pos, freqs_cis, mask, adapter[i].type(model.llamatype), vqa_video_start)\n",
    "            vqa_h = model.norm(vqa_h)\n",
    "            vqa_output = model.output(vqa_h)\n",
    "            vqa_output = vqa_output.reshape(-1, model.vocab_size)\n",
    "\n",
    "            next_token = vqa_output[-1,:].argmax()\n",
    "            tokens.append(next_token.item())\n",
    "            token_emb = model.tok_embeddings(next_token.unsqueeze(0))\n",
    "            input_embedding = torch.cat([input_embedding,token_emb],dim=0)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Action Sequence': 42.0, 'Action Prediction': 38.5, 'Action Antonym': 35.0, 'Fine-grained Action': 34.5, 'Unexpected Action': 66.0, 'Object Existence': 52.5, 'Object Interaction': 47.5, 'Object Shuffle': 28.000000000000004, 'Moving Direction': 22.0, 'Action Localization': 37.5, 'Scene Transition': 81.0, 'Action Count': 38.0, 'Moving Count': 24.0, 'Moving Attribute': 42.5, 'State Change': 41.5, 'Fine-grained Pose': 28.499999999999996, 'Character Order': 34.0, 'Egocentric Navigation': 23.5, 'Episodic Reasoning': 49.0, 'Counterfactual Inference': 30.5, 'Avg': 39.800000000000004}\n"
     ]
    }
   ],
   "source": [
    "final_res = dict()\n",
    "correct = 0\n",
    "total = 0\n",
    "for k, v in acc_dict.items():\n",
    "    final_res[k] = v[0] / v[1] * 100\n",
    "    correct += v[0]\n",
    "    total += v[1]    \n",
    "final_res['Avg'] = correct / total * 100\n",
    "\n",
    "print(final_res)\n",
    "\n",
    "with open(\"upload_leaderboard_7B_111.json\", \"w\") as f:\n",
    "    json.dump(final_res, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
